{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc6df11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/envs/bigdata/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/15 19:22:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/opt/conda/envs/bigdata/lib/python3.8/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf()\n",
    "\n",
    "conf.set('spark.ui.proxyBase', '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4041')\n",
    "conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "conf.set('spark.driver.memory','4g')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "spark = pyspark.SQLContext.getOrCreate(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e5fc6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Items</th><th>date</th><th>hour</th><th>count</th></tr>\n",
       "<tr><td>Bread</td><td>2016-10-31</td><td>10</td><td>2</td></tr>\n",
       "<tr><td>Medialuna</td><td>2016-04-11</td><td>8</td><td>1</td></tr>\n",
       "<tr><td>Granola</td><td>2016-05-11</td><td>8</td><td>1</td></tr>\n",
       "<tr><td>Farm House</td><td>2016-06-11</td><td>11</td><td>1</td></tr>\n",
       "<tr><td>Tartine</td><td>2016-08-11</td><td>14</td><td>1</td></tr>\n",
       "<tr><td>Truffles</td><td>2016-12-11</td><td>11</td><td>2</td></tr>\n",
       "<tr><td>Bread</td><td>2016-12-11</td><td>16</td><td>2</td></tr>\n",
       "<tr><td>Muffin</td><td>2016-11-15</td><td>16</td><td>1</td></tr>\n",
       "<tr><td>Pastry</td><td>2016-11-18</td><td>10</td><td>2</td></tr>\n",
       "<tr><td>Coffee</td><td>2016-11-28</td><td>8</td><td>1</td></tr>\n",
       "<tr><td>Bread</td><td>2016-07-12</td><td>12</td><td>3</td></tr>\n",
       "<tr><td>Vegan mincepie</td><td>2016-12-12</td><td>11</td><td>1</td></tr>\n",
       "<tr><td>Hot chocolate</td><td>2016-12-13</td><td>10</td><td>1</td></tr>\n",
       "<tr><td>Coffee</td><td>2016-12-14</td><td>11</td><td>8</td></tr>\n",
       "<tr><td>Pastry</td><td>2016-12-16</td><td>11</td><td>1</td></tr>\n",
       "<tr><td>Hot chocolate</td><td>2016-12-19</td><td>9</td><td>2</td></tr>\n",
       "<tr><td>Sandwich</td><td>2016-12-20</td><td>9</td><td>1</td></tr>\n",
       "<tr><td>Coffee</td><td>2016-12-21</td><td>10</td><td>12</td></tr>\n",
       "<tr><td>Sandwich</td><td>2016-12-21</td><td>12</td><td>1</td></tr>\n",
       "<tr><td>Tea</td><td>2016-12-24</td><td>10</td><td>1</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------+----------+----+-----+\n",
       "|         Items|      date|hour|count|\n",
       "+--------------+----------+----+-----+\n",
       "|         Bread|2016-10-31|  10|    2|\n",
       "|     Medialuna|2016-04-11|   8|    1|\n",
       "|       Granola|2016-05-11|   8|    1|\n",
       "|    Farm House|2016-06-11|  11|    1|\n",
       "|       Tartine|2016-08-11|  14|    1|\n",
       "|      Truffles|2016-12-11|  11|    2|\n",
       "|         Bread|2016-12-11|  16|    2|\n",
       "|        Muffin|2016-11-15|  16|    1|\n",
       "|        Pastry|2016-11-18|  10|    2|\n",
       "|        Coffee|2016-11-28|   8|    1|\n",
       "|         Bread|2016-07-12|  12|    3|\n",
       "|Vegan mincepie|2016-12-12|  11|    1|\n",
       "| Hot chocolate|2016-12-13|  10|    1|\n",
       "|        Coffee|2016-12-14|  11|    8|\n",
       "|        Pastry|2016-12-16|  11|    1|\n",
       "| Hot chocolate|2016-12-19|   9|    2|\n",
       "|      Sandwich|2016-12-20|   9|    1|\n",
       "|        Coffee|2016-12-21|  10|   12|\n",
       "|      Sandwich|2016-12-21|  12|    1|\n",
       "|           Tea|2016-12-24|  10|    1|\n",
       "+--------------+----------+----+-----+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 1 Code\n",
    "import pyspark.sql.functions as func\n",
    "Bakery = spark.read.option(\"inferSchema\", \"true\").option(\"header\",\"true\").csv(\"/home/jovyan/shared/hw2_data/Bakery.csv\")\n",
    "Bakery_split = Bakery.withColumn(\"date\", func.to_date(func.col(\"DateTime\"))).withColumn(\"hour\", func.hour(func.col(\"DateTime\")))\n",
    "Bakery_split.groupBy(\"Items\",\"date\",\"hour\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd712cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:========================================>               (35 + 2) / 48]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------------------+\n",
      "|  Daypart|DayType|               words|\n",
      "+---------+-------+--------------------+\n",
      "|  Evening|Weekday|[Coffee, Bread, Tea]|\n",
      "|  Morning|Weekday|[Coffee, Pastry, ...|\n",
      "|Afternoon|Weekday|[Coffee, Bread, Tea]|\n",
      "|    Night|Weekday|[Valentine's card...|\n",
      "|  Evening|Weekend|[Coffee, Tshirt, ...|\n",
      "|  Morning|Weekend|[Coffee, Pastry, ...|\n",
      "|Afternoon|Weekend|[Coffee, Bread, Tea]|\n",
      "|    Night|Weekend|[Scandinavian, Ho...|\n",
      "+---------+-------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Question 2 Code\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "from pyspark.sql import Row\n",
    "x = Bakery.groupBy(\"Daypart\",\"Items\", \"DayType\").count().sort(\"Daypart\",\"DayType\",func.desc(\"count\"))\n",
    "Daypart = Bakery.select(\"Daypart\").distinct().rdd.map(lambda r: r[0]).collect()\n",
    "DayType = Bakery.select(\"DayType\").distinct().rdd.map(lambda r: r[0]).collect()\n",
    "\n",
    "MyQ2Schema = StructType([StructField(\"Daypart\", StringType(), True),\n",
    "                        StructField(\"food\", StringType(), True),\n",
    "                        StructField(\"DayType\", StringType(), True)])\n",
    "\n",
    "length = len(Daypart) * len(DayType)\n",
    "test = x.filter((func.col(\"DayType\") == DayType[0]) & (func.col(\"Daypart\") == Daypart[0])).drop('count').take(3)\n",
    "QuestionTwoDataFrame = spark.createDataFrame([test[0]],MyQ2Schema)\n",
    "newRow = spark.createDataFrame([test[1]])\n",
    "QuestionTwoDataFrame =  QuestionTwoDataFrame.union(newRow)\n",
    "newRow = spark.createDataFrame([test[2]])\n",
    "QuestionTwoDataFrame =  QuestionTwoDataFrame.union(newRow)\n",
    "\n",
    "for i in range(1,len(Daypart)):\n",
    "    test = x.filter((func.col(\"DayType\") == DayType[0]) & (func.col(\"Daypart\") == Daypart[i])).drop('count').take(3)\n",
    "    for j in range(3):\n",
    "        newRow = spark.createDataFrame([test[j]])\n",
    "        QuestionTwoDataFrame =  QuestionTwoDataFrame.union(newRow)    \n",
    "\n",
    "for i in range(len(Daypart)):\n",
    "    test = x.filter((func.col(\"DayType\") == DayType[1]) & (func.col(\"Daypart\") == Daypart[i])).drop('count').take(3)\n",
    "    for j in range(3):\n",
    "        newRow = spark.createDataFrame([test[j]])\n",
    "        QuestionTwoDataFrame =  QuestionTwoDataFrame.union(newRow)    \n",
    "\n",
    "FinalQuestionTwo = QuestionTwoDataFrame.groupBy('Daypart', 'DayType').agg(func.collect_set('food').alias('words')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a6b9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Rpt_Area_Desc</th><th>count</th></tr>\n",
       "<tr><td>Bed&amp;Breakfast Home</td><td>4</td></tr>\n",
       "<tr><td>Summer Camps</td><td>4</td></tr>\n",
       "<tr><td>Institutions</td><td>30</td></tr>\n",
       "<tr><td>Local Confinement</td><td>2</td></tr>\n",
       "<tr><td>Mobile Food</td><td>147</td></tr>\n",
       "<tr><td>School Buildings</td><td>89</td></tr>\n",
       "<tr><td>null</td><td>13</td></tr>\n",
       "<tr><td>Summer Food</td><td>242</td></tr>\n",
       "<tr><td>Swimming Pools</td><td>420</td></tr>\n",
       "<tr><td>Day Care</td><td>173</td></tr>\n",
       "<tr><td>Tattoo Establishm...</td><td>32</td></tr>\n",
       "<tr><td>Residential Care</td><td>154</td></tr>\n",
       "<tr><td>Bed&amp;Breakfast Inn</td><td>2</td></tr>\n",
       "<tr><td>Adult Day Care</td><td>5</td></tr>\n",
       "<tr><td>Lodging</td><td>62</td></tr>\n",
       "<tr><td>Food Service</td><td>1093</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+-----+\n",
       "|       Rpt_Area_Desc|count|\n",
       "+--------------------+-----+\n",
       "|  Bed&Breakfast Home|    4|\n",
       "|        Summer Camps|    4|\n",
       "|        Institutions|   30|\n",
       "|   Local Confinement|    2|\n",
       "|         Mobile Food|  147|\n",
       "|    School Buildings|   89|\n",
       "|                null|   13|\n",
       "|         Summer Food|  242|\n",
       "|      Swimming Pools|  420|\n",
       "|            Day Care|  173|\n",
       "|Tattoo Establishm...|   32|\n",
       "|    Residential Care|  154|\n",
       "|   Bed&Breakfast Inn|    2|\n",
       "|      Adult Day Care|    5|\n",
       "|             Lodging|   62|\n",
       "|        Food Service| 1093|\n",
       "+--------------------+-----+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 3 Code\n",
    "Resturants = spark.read.option(\"inferSchema\", \"true\").option(\"header\",\"true\").option(\"delimiter\",\";\").csv(\"/home/jovyan/shared/hw2_data/Restaurants_in_Durham_County_NC.csv\")\n",
    "Resturants.groupBy(\"Rpt_Area_Desc\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ab9ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/15 19:24:02 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1980, 1981\n",
      " Schema: _c0, 1980, 1981\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1981, 1982\n",
      " Schema: _c0, 1981, 1982\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1982, 1983\n",
      " Schema: _c0, 1982, 1983\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1983, 1984\n",
      " Schema: _c0, 1983, 1984\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1984, 1985\n",
      " Schema: _c0, 1984, 1985\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:05 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1985, 1986\n",
      " Schema: _c0, 1985, 1986\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:05 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1986, 1987\n",
      " Schema: _c0, 1986, 1987\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:06 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1987, 1988\n",
      " Schema: _c0, 1987, 1988\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:07 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1988, 1989\n",
      " Schema: _c0, 1988, 1989\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:07 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1989, 1990\n",
      " Schema: _c0, 1989, 1990\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1990, 1991\n",
      " Schema: _c0, 1990, 1991\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1991, 1992\n",
      " Schema: _c0, 1991, 1992\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1992, 1993\n",
      " Schema: _c0, 1992, 1993\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1993, 1994\n",
      " Schema: _c0, 1993, 1994\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1994, 1995\n",
      " Schema: _c0, 1994, 1995\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1995, 1996\n",
      " Schema: _c0, 1995, 1996\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1996, 1997\n",
      " Schema: _c0, 1996, 1997\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1997, 1998\n",
      " Schema: _c0, 1997, 1998\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1998, 1999\n",
      " Schema: _c0, 1998, 1999\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:12 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 1999, 2000\n",
      " Schema: _c0, 1999, 2000\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:12 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 2000, 2001\n",
      " Schema: _c0, 2000, 2001\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 2001, 2002\n",
      " Schema: _c0, 2001, 2002\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 2002, 2003\n",
      " Schema: _c0, 2002, 2003\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:14 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 2003, 2004\n",
      " Schema: _c0, 2003, 2004\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:14 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 2004, 2005\n",
      " Schema: _c0, 2004, 2005\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 2005, 2006\n",
      " Schema: _c0, 2005, 2006\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 2006, 2007\n",
      " Schema: _c0, 2006, 2007\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:16 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 2007, 2008\n",
      " Schema: _c0, 2007, 2008\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:16 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 2008, 2009\n",
      " Schema: _c0, 2008, 2009\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "22/03/15 19:24:16 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , 2009, 2010\n",
      " Schema: _c0, 2009, 2010\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------+\n",
      "|Year|             Country| Percentage|\n",
      "+----+--------------------+-----------+\n",
      "|1981|    [Western Sahara]|0.121331826|\n",
      "|1982|    [Western Sahara]|0.111151055|\n",
      "|1983|     [French Guiana]| 0.14285715|\n",
      "|1984|             [Qatar]|0.109640576|\n",
      "|1985|     [French Guiana]|      0.125|\n",
      "|1986|             [Qatar]|0.087717324|\n",
      "|1987|     [French Guiana]| 0.11111111|\n",
      "|1988|    [Cayman Islands]| 0.11010421|\n",
      "|1989|[United Arab Emir...| 0.06119858|\n",
      "|1990|          [Djibouti]| 0.12824048|\n",
      "|1991|            [Jordan]|  0.1127394|\n",
      "|1992|            [Kuwait]| 0.48633438|\n",
      "|1993|       [Afghanistan]| 0.13224594|\n",
      "|1994|       [Afghanistan]|0.087276615|\n",
      "|1995|           [Burundi]| 0.07222489|\n",
      "|1996|            [Rwanda]| 0.19614178|\n",
      "|1997|[Falkland Islands...|      0.215|\n",
      "|1998|           [Liberia]|  0.1201745|\n",
      "|1999|[Falkland Islands...| 0.07692308|\n",
      "|2000|        [Montserrat]| 0.16863905|\n",
      "+----+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question 4 Code\n",
    "\n",
    "from pyspark.sql.types import FloatType\n",
    "Time_Data = spark.read.option(\"inferSchema\", \"true\").option(\"header\",\"true\").csv(\"/home/jovyan/shared/hw2_data/populationbycountry19802010millions.csv\")\n",
    "names = Time_Data.columns\n",
    "\n",
    "names = Time_Data.columns\n",
    "for i in range(1,len(names) - 1):\n",
    "    Time_Data = Time_Data.withColumn(names[i + 1] + \" subtract \" + names[i], ((func.col(names[i + 1]) - func.col(names[i]))/func.col(names[i])))\n",
    "\n",
    "colnames = Time_Data.columns\n",
    "\n",
    "top_percent = Time_Data.groupby().max('1981 subtract 1980').first().asDict()['max(1981 subtract 1980)']\n",
    "country = Time_Data.where(func.col('1981 subtract 1980') == top_percent).select(func.col('_c0')).rdd.map(lambda r: r[0]).collect()\n",
    "\n",
    "MyQ4Schema = StructType([StructField(\"Year\", StringType(), True),\n",
    "                        StructField(\"Country\", StringType(), True),\n",
    "                        StructField(\"Percentage\", FloatType(), True)])\n",
    "\n",
    "firstRow = Row(colnames[2],country,top_percent)\n",
    "QuestionFourDataFrame = spark.createDataFrame([firstRow],MyQ4Schema)\n",
    "\n",
    "\n",
    "for i in range(29):\n",
    "    year = colnames[i+3]\n",
    "    subtraction = colnames[i + 33]\n",
    "    top_percent = Time_Data.groupby().max(subtraction).first().asDict()['max('+subtraction+')']\n",
    "    country = Time_Data.where(func.col(subtraction) == top_percent).select(func.col(colnames[0])).rdd.map(lambda r: r[0]).collect()\n",
    "    new_row = Row(year,country,top_percent)\n",
    "    new_row_df = spark.createDataFrame([new_row],MyQ4Schema)\n",
    "    QuestionFourDataFrame = QuestionFourDataFrame.union(new_row_df)\n",
    "\n",
    "QuestionFourDataFrame.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ced4f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 175:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|        some|42889|\n",
      "|        hope| 5880|\n",
      "|       still|28742|\n",
      "|  lieutenant| 1370|\n",
      "|         fog|  925|\n",
      "|    tortured|  288|\n",
      "|       trail| 1209|\n",
      "|       those|18840|\n",
      "|       inner| 2029|\n",
      "|    affixing|    2|\n",
      "|         few|18027|\n",
      "|     reddish|  224|\n",
      "|        guts|  509|\n",
      "|   arguments|  315|\n",
      "|   connected|  723|\n",
      "|accumulation|   98|\n",
      "|      harder|  884|\n",
      "|   traveling|  746|\n",
      "|     skidded|  186|\n",
      "|   amplifier|  123|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Question 5 Code\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "import re\n",
    "Text_file = spark.read.option(\"inferSchema\", \"true\").option(\"lineSep\",\".\").text(\"/home/jovyan/shared/hw2_data/internet_archive_scifi_v3.txt\")\n",
    "Text_file=Text_file.withColumn(\"value\", func.lower(func.col(\"value\")))\n",
    "Text_file = Text_file.withColumn(\"value\",regexp_replace(\"value\", \"[^0-9a-zA-Z$]+\", \" \"))\n",
    "Text_file = Text_file.withColumn('word', func.explode(func.split(func.col('value'), '\\s+'))).groupBy('word')\\\n",
    "    .count()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa7451f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "RDD is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m bigrams \u001b[38;5;241m=\u001b[39m Text_file2\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mflatMap(\u001b[38;5;28;01mlambda\u001b[39;00m xs: (\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(xs, xs[\u001b[38;5;241m1\u001b[39m:])))\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#bigrams.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mbigrams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/bigdata/lib/python3.8/site-packages/pyspark/sql/session.py:66\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampleRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=1)]\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampleRatio\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/bigdata/lib/python3.8/site-packages/pyspark/sql/session.py:675\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(\n\u001b[1;32m    674\u001b[0m         data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/bigdata/lib/python3.8/site-packages/pyspark/sql/session.py:698\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    695\u001b[0m     prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m obj: obj\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[0;32m--> 698\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[0;32m/opt/conda/envs/bigdata/lib/python3.8/site-packages/pyspark/sql/session.py:486\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 486\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    488\u001b[0m     rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[0;32m/opt/conda/envs/bigdata/lib/python3.8/site-packages/pyspark/sql/session.py:460\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inferSchema\u001b[39m(\u001b[38;5;28mself\u001b[39m, rdd, samplingRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;124;03m    Infer schema from an RDD of Row, dict, or tuple.\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m    :class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m first:\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first row in RDD is empty, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    463\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan not infer schema\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/bigdata/lib/python3.8/site-packages/pyspark/rdd.py:1591\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1591\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: RDD is empty"
     ]
    }
   ],
   "source": [
    "Text_file2 = spark.read.option(\"inferSchema\", \"true\").option(\"lineSep\",\".\").text(\"/home/jovyan/shared/hw2_data/internet_archive_scifi_v3.txt\")\n",
    "Text_file2 =Text_file2.withColumn(\"value\", func.lower(func.col(\"value\")))\n",
    "Text_file2 = Text_file2.withColumn(\"value\",regexp_replace(\"value\", \"[^0-9a-zA-Z$]+\", \" \"))\n",
    "Text_file2 = Text_file2.na.drop()\n",
    "bigrams = Text_file2.rdd.flatMap(lambda xs: (tuple(x) for x in zip(xs, xs[1:]))).map(lambda x: (x))\n",
    "\n",
    "#bigrams.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "test = bigrams.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aab70a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = bigrams.collect()\n",
    "[f(e) for e in r]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata]",
   "language": "python",
   "name": "conda-env-bigdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
